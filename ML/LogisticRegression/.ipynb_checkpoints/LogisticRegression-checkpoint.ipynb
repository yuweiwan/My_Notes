{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归Logistic Regression\n",
    "逻辑回归用于分类，说白了就是线性回归的形式加入sigmoid函数中。为什么不用线性回归找分界线呢？线性回归更容易受到异常值的影响， 而LR对异常值有较好的稳定性。同时,因为sigmoid函数的特性,它输出的结果也不再是预测结果,而是一个值预测为正例的概率。也就是说，logistic 回归不是对所有数据点进行拟合，而是对 label 为1的样本的概率的拟合。\n",
    "- 本质： 极大似然估计\n",
    "- 激活函数：Sigmoid\n",
    "- 代价函数：交叉熵\n",
    "\n",
    "公式：$$ h_\\theta(X) = sigmoid(X\\theta) = \\frac{1}{1 + e^{-X\\theta}} $$\n",
    "<br>\n",
    "$h_\\theta(X)$维度为$m*1$，$X$为特征矩阵维度为$m*n$，$\\theta$为模型系数维度为$n*1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推导：\n",
    "逻辑回归解决的是分类问题。为什么选择sigmoid函数呢，有人说是为了将线性回归的值压缩到0-1之间，但是符合这个条件的函数有很多，为什么偏偏选择了sigmoid函数。<br>\n",
    "一句话解释：因为作为广义线性模型（GLM）中的一类，逻辑回归的连接函数的 canonical 形式就是 sigmoid函数\n",
    "# 为什么选择sigmoid函数呢\n",
    "# ？？？\n",
    "https://zhuanlan.zhihu.com/p/35036985\n",
    "\n",
    "\n",
    "首先sigmoid函数为$g(z)=\\frac{1}{1+e^{-z}}$，它有一个性质是导数为$g(z)(1-g(z))$  \n",
    "LR 满足伯努利分布： $$ P(Y=1|x; \\theta) = h_{\\theta}(x)\\\\ \n",
    "P(Y=0|x; \\theta) = 1 - h_{\\theta}(x)\\\\ \n",
    "p(y|x; \\theta) = (h_{\\theta}(x))^y (1-h_{\\theta}(x))^{1-y} $$\n",
    "\n",
    "# 为什么要用交叉熵作为损失函数呢\n",
    "# ？？？\n",
    "损失函数（极大似然）: 对于训练数据集，特征数据 $x={x_1, ...x_m}$ 和其对应的分类标签 $y = {y_1,...y_m}$ ， 假设m个样本是相互独立的，那么极大似然函数为： \n",
    "$$ \\begin{align} L(\\theta) &= \\prod_{i=1}^m p(y^{(i)}|x(i);\\theta)\\\\  &= \\prod_{i=1}^m (h_{\\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\\theta}(x^{(i)}))^{1-y^{(i)}} \\end{align} $$\n",
    "那么它的 log 似然为： \n",
    "$$ \\begin{align} l(\\theta)=log L(\\theta ) = \\sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)})) \\end{align} $$\n",
    "参数优化（梯度上升） \n",
    "$$ \\begin{align} \\frac{\\partial l(\\theta)}{\\partial \\theta_j} &= (y \\frac{1}{g(\\theta^Tx)} - (1-y) \\frac{1}{1 -g(\\theta^Tx)}) \\frac{\\delta g(\\theta^Tx)}{\\delta \\theta_j} \\\\ &= (y \\frac{1}{g(\\theta^Tx)} - (1-y)\\frac{1}{1 -g(\\theta^Tx)} ) g(\\theta^Tx)(1-g(\\theta^Tx)) \\frac{\\delta \\theta^Tx}{\\theta_j} \\\\ &= (y (1 - g(\\theta^Tx)) - (1-y) g(\\theta^Tx)) x_j \\\\ &= [y - h_{\\theta} (x)]x_j \\ \\end{align} $$\n",
    "\n",
    "$$ \\begin{align} \\theta_j &= \\theta_j + \\alpha \\frac{\\partial L(\\theta)}{\\partial \\theta} = \\theta_j + \\alpha [y^{(i)} - h_{\\theta} (x^{(i)})]x_j^{(i)}\n",
    "\\end{align} $$\n",
    "\n",
    "损失函数： $$ J(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)}log(h_\\theta(x^{(i)})) + (1-y^{(i)}) log(1 - h_\\theta(x^{(i)})) \\right] $$\n",
    "\n",
    "其损失函数实际意义为  \n",
    "1）根据极大似然法将这些条件概率连乘取对数取反得到逻辑回归的损失函数，意义在于希望能够最大化这些样本出现的概率。  \n",
    "2）二元交叉熵$L_{logistic}(\\hat y,y)=-y\\log \\hat y-(1-y)\\log(1-\\hat y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR 如何实现多分类？\n",
    "\n",
    "方式1： 修改逻辑回归的损失函数，使用softmax函数构造模型解决多分类问题，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。  \n",
    "方式2： 根据每个类别都建立一个二分类器，本类别的样本标签定义为0，其它分类样本标签定义为1，则有多少个类别就构造多少个逻辑回归分类器。\n",
    "若所有类别之间有明显的互斥则使用softmax分类器，若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。\n",
    "\n",
    "### LR 为何要对特征进行离散化\n",
    "\n",
    "非线性：逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型**引入非线性**，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；  \n",
    "鲁棒性：离散后对异常数据有很强的**鲁棒性**：比如一个特征是年龄>30是1，否则0。如果没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；  \n",
    "方便交叉与特征组合： 离散化后可以进行特征交叉，由$M+N$个变量变为$M*N$个变量，进一步引入非线性，提升表达能力。  \n",
    "稳定性： 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。  \n",
    "简化模型： 简化模型，降低了**模型过拟合**的风险。**稀疏向量内积乘法运算速度快**，计算结果方便存储，容易扩展。   \n",
    "李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。   \n",
    "通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 \n",
    "\n",
    "### LR为什么对切斜的数据特别敏感(正负例数据比例相差悬殊时预测效果不好)\n",
    "\n",
    "原因来自于逻辑回归交叉熵损失函数是通过最大似然估计来推导出的。使用最大似然估计来推导损失函数,那无疑,我们得到的结果就是所有样本被预测正确的最大概率。那么,当我们的业务场景是来预测垃圾邮件,预测黄色图片时,我们数据中99%的都是负例(不是垃圾邮件不是黄色图片),如果有两组w,第一组为所有的负例都预测正确,而正利预测错误,正确率为99%,第二组是正利预测正确了,但是负例只预测出了97个,正确率为98%.此时我们算法会认为第一组w是比较好的.但实际我们业务需要的是第二组,因为正例检测结果才是业务的根本。  \n",
    "此时我们需要对数据进行欠采样/重采样来让正负例保持一个差不多的平衡,或者使用树型算法来做分类.一般树型分类的算法对数据倾斜并不是很敏感,但我们在使用的时候还是要对数据进行欠采样/重采样来观察结果是不是有变好.\n",
    "\n",
    "### LR和softmax的关系\n",
    "\n",
    "LR针对二分类，softmax针对多分类。Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的k个可能值进行了累加。二者条件公式不同，逻辑回归公式是$\\frac{1}{1+\\exp(-\\theta^Tx)}$，softmax回归公式是$\\frac{e^{\\theta^T_jx^{i}}}{\\sum^k_{i=1}e^{\\theta^T_lx^{i}}}$。当用softmax二分类时，退化成LR。  \n",
    "在应用时，如果多个类别且它们互斥，那么用softmax比较好，如果不互斥，那用多个LR比较好。\n",
    "\n",
    "### LR的特点，什么时候用，以及如何提升\n",
    "\n",
    "#### 优点：\n",
    "**可解释性好**。从特征的权重可以看到不同的特征对最后结果的影响。<br> \n",
    "**效果不错**。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差。<br> \n",
    "**速度较快**。分类时计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，这样我们可以在短时间内迭代好几个版本。 <br>\n",
    "**资源占用小**,尤其是内存。因为只需要存储各个维度的特征值。 <br>\n",
    "**方便输出结果调整**，逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。 <br>\n",
    "可以满足**线上实时**的需求因为lr可以很方便的做成线上学习（online learning）的形式。\n",
    "#### 缺点：\n",
    "**拟合能力较弱**。因为形式简单，很难拟合真实分布。\n",
    "#### 使用场景：\n",
    "最基本的：**输出类别服从伯努利二项分布，样本大致线性可分**。<br>\n",
    "**不必在意特征间相关性**的情景（如果强调可解释性，特征相关性会影响结果，因为相关性太高的特征之间会接近“平分”梯度的更新值，如果不在意解释性，那么其实也不太需要额外处理特征共线性的问题因为共线性不影响模型的使用）。<br>\n",
    "后续有**新数据**。（因为lr很容易做成在线学习的形式）。<br>\n",
    "需要一个快，好解释的**baseline**时，特别是金融机构这类对于可解释性比较看重的（可以使用业务层面的丰富的先验知识来辅助模型效果会更好也更直观。）\n",
    "#### 提升：\n",
    "特征离散化、特征交叉、调参（学习率和正则化系数等）、gbdt提取高阶特征、使用正则化等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归代码实战\n",
    "分别使用sklearn的包，梯度上升来实现乳腺癌数据集的二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 从sklearn加载乳腺癌数据集\n",
    "cancer=datasets.load_breast_cancer()\n",
    "cancer_X=cancer.data\n",
    "cancer_y=cancer.target\n",
    "# 按0.3分割训练集和测试集\n",
    "x_train,x_test,y_train, y_test=train_test_split(cancer_X,cancer_y,test_size=0.3)\n",
    "# 对特征数据进行标准化处理（预测值为类别0、1所以无需标准化）\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# 使用sklearn的包预测，默认使用L2正则化避免过拟合，C=1.0表示正则力度\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression(C=1.0) \n",
    "lg.fit(x_train, y_train)\n",
    " \n",
    "# 回归系数\n",
    "weights=lg.coef_\n",
    "# 进行预测\n",
    "y_predict = lg.predict(x_test) \n",
    "print(\"准确率：\", lg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# 使用梯度上升求解\n",
    "import numpy as np\n",
    "from numpy import matrix\n",
    "m = len(y_train)\n",
    "X = np.hstack((np.ones((m,1)),x_train))\n",
    "\n",
    "def sigmoid(ix):\n",
    "    return 1/(1+np.exp(-ix))\n",
    "\n",
    "def gradient_ascent(x, y, alpha, iters):    \n",
    "    m = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    # x 是 m * n 矩阵, m 个样本，n-1个特征, 1列1\n",
    "    x = matrix(x)\n",
    "    # y 是 m * 1 向量\n",
    "    y = matrix(y).transpose()\n",
    "    # theta 是 n * 1 向量\n",
    "    theta = np.matrix(np.zeros((n, 1)))\n",
    "\n",
    "    for i in range(iters):\n",
    "        h = sigmoid(x*theta)\n",
    "        for j in range(n):\n",
    "            theta[j,0]=theta[j,0] + alpha*x[:,j].transpose()*(y-h)\n",
    "    return theta\n",
    "\n",
    "def cal_acc(theta, x_test, y_test):\n",
    "    m = len(y_test)\n",
    "    X_test = np.hstack((np.ones((m,1)),x_test))\n",
    "    h = sigmoid(X_test*theta)\n",
    "    predict = []\n",
    "    for i in range(m):\n",
    "        if h[i]>0.5:\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    count = 0\n",
    "    for i in range(m):\n",
    "        if predict[i]!=y_test[i]:\n",
    "            count+=1\n",
    "    print(\"准确率:\", (m-count)/m)\n",
    "    \n",
    "theta = gradient_ascent(X,y_train,0.001,500)\n",
    "cal_acc(theta, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference：\n",
    "\n",
    "逻辑回归面经整理——from牛客 https://zhuanlan.zhihu.com/p/68946840 NLP面试题：https://github.com/songyingxin/NLPer-Interview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
