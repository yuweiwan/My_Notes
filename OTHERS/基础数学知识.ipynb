{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础数学知识\n",
    "\n",
    "## 信息论\n",
    "\n",
    "事件发生的概率越大，那么事件的信息量就越小， 事件的概率与事件的信息量之间成反比。  \n",
    "定义一个事件x的**自信息**为：$I(x) = -log(p(x))$  \n",
    "当该对数的底数为自然对数 e 时，单位为奈特（nats）；当以 2 为底数时，单位为比特（bit）或香农（shannons）.\n",
    "\n",
    "- 信息熵：编码方案完美时，最短平均编码长度的是多少。\n",
    "- 交叉熵：编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度的是多少。(平均编码长度 = 最短平均编码长度 + 一个增量)\n",
    "- 相对熵：编码方案不一定完美时，平均编码长度相对于最小值的增加值。（即上面那个增量）\n",
    "\n",
    "\n",
    "**信息熵**(entropy):  \n",
    "信息熵是对平均不确定性的度量，本质上是**所有事件的信息量的期望**， 对整个概率分布中的不确定性总量进行量化：    \n",
    "$$H(X) = E_{X}[I(x)]=-\\sum_{x \\in X} p(x)log(p(x)) \\quad \\text{X表示所有事件}$$\n",
    "信息论中，记 $0\\log0 = 0$  \n",
    "\n",
    "- 当且仅当某个 $P(X_i)=1$，其余的都等于0时， H(X)= 0。\n",
    "- 当且仅当某个$P(X_i)=1/n，i=1， 2，……， n$时，$H(X)$ 有极大值 log n。\n",
    "\n",
    "熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。\n",
    "\n",
    "**相对熵（KL散度）与交叉熵**：  \n",
    "对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，可以使用KL散度来衡量这两个分布的差异。在离散型变量的情况下， KL 散度衡量的是：当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。  \n",
    "P对Q的**相对熵（KL散度）**为：\n",
    "$$D_{KL}(P||Q) =\\sum_{x \\in X}P(x)log(\\frac{P(x)}{Q(x)})$$\n",
    "性质：\n",
    "- 非负: KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的.\n",
    "- 不对称：$D_p(q) != D_q(p)$\n",
    "\n",
    "设 $P(x), Q(x)$ 为 $X$ 中取值的两个概率分布，则 $P$ 对 $Q$ 的**交叉熵**为：\n",
    "$$H(P,Q) = -\\sum_{x \\in X}P(x)\\log Q(x)$$\n",
    "\n",
    "相对熵（KL散度）和交叉熵的关系：\n",
    "- 针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度，因为 Q 并不参与被省略的那一项。  \n",
    "交叉熵=信息熵+相对熵\n",
    "  $$\n",
    "  H(P,Q) = H(P) + D_{KL}(P||Q)\n",
    "  $$\n",
    "\n",
    "- 最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。\n",
    "\n",
    "交叉熵在 ML 中等效于相对熵【作用：用来评估，当前训练得到的概率分布，与真实分布有多么大的差异】  \n",
    "因为与相对熵只差一个 分布 P 的信息熵，若 P 是固定的分布，与训练无关；  \n",
    "Q 是估计的分布，应尽量等于 P。二者一致时，交叉熵就等于 P 的熵\n",
    "\n",
    "\n",
    "**联合熵与条件熵**：  \n",
    "- 联合熵 $H(X, Y)$：两个随机变量X，Y的联合分布。\n",
    "- 条件熵 $H(Y|X) $：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。\n",
    "$$\\begin{align}\n",
    "H(Y|X) &= H(X,Y) - H(X)\\\\\n",
    "&= -\\sum_{x,y} p(x,y) log \\, p(x,y) + \\sum_x p(x) log \\, p(x) \\\\\n",
    "&= -\\sum_{x,y} p(x,y) log \\, p(x,y) +  \\sum_x (\\sum_y p(x,y)) \\, log \\, p(x) \\qquad \\text{边缘分布 p(x) 等于联合分布 p(x,y) 的和} \\\\\n",
    "&= -\\sum_{x,y} p(x,y) log \\, p(x,y) +  \\sum_{x,y} p(x,y) \\, log \\, p(x) \\\\ \n",
    "&= -\\sum_{x,y} p(x,y) log \\frac{p(x,y)}{p(x)} \\\\\n",
    "&= -\\sum_{x,y} p(x,y) log p(y|x)\n",
    "\\end{align}$$\n",
    "\n",
    "**互信息**：  \n",
    "- $I(X, Y)$ ：两个随机变量X，Y的互信息 为**X，Y的联合分布**和**各自独立分布乘积**的**相对熵**。\n",
    "- 互信息取值为非负。当X、Y相互独立时，$I(X,Y)$ 最小为0。\n",
    "$$I(X,Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x,y) log( \\frac{p(x,y)}{p(x)p(y)})\\\\\n",
    "I(X, Y) = D(P(X,Y) || P(X)P(Y))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
