{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础概念包括偏差bias与方差variance，最大似然估计MLE与最大后验概率MAP，正则化Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 偏差与方差\n",
    "偏差(bias)：\\\n",
    "衡量了算法的期望预测与真实结果的偏离程度，即算法本身的拟合能力，偏差越大，拟合能力越差。偏差=最佳误差率+可避免误差，最佳误差率需要人为估计。\n",
    "\n",
    "方差(variance)：\\\n",
    "衡量训练集变动导致的学习性能的变化，即数据扰动的影响，方差越大，模型稳定性越差。\n",
    "\n",
    "举例：\\\n",
    "训练集错误率15%,开发集错误率16%，那么可以理解为偏差是15%，方差是16%-15%=1%。\n",
    "\n",
    "如何减少偏差：\\\n",
    "使用更复杂的模型，或改善特征（需要专家知识），或减少正则化（非必要不提倡）\n",
    "\n",
    "如何减少方差：\\\n",
    "增加训练数据，或提前终止，或加入正则化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最大似然估计与最大后验概率\n",
    "要理解两者区别需要理解贝叶斯思想，首先来看贝叶斯公式：\n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\neg A)P(\\neg A)}$$\n",
    "\n",
    "$P(A)$是先验概率prior，$P(B)$是边际概率marginal，$P(A|B)$是后验概率posterior，$P(B|A)$是似然概率Likelihood。\n",
    "\n",
    "这个公式描述了你相信一个证据的程度是多少。比如A是汽车被砸了，B是警报响了。现在警报响了，那么原因是汽车被砸的概率有多大呢？\n",
    "\n",
    "如果让$P(\\neg A)=0$即把其他原因比如小孩子踢球碰到了，警报器故障了等都排除，那么铁定是汽车被砸了。这个角度说明做判断的时候要考虑到所有因素。\n",
    "\n",
    "我们知道汽车被砸后警报大概率会响，即似然概率$P(B|A)$很大，但是如果本身汽车被砸这件事发生概率很低，即先验概率$P(A)$特别小的话，那么从公式可以看到后验概率还是不会特别大。从这个角度说明还要考虑到因素本身的发生概率。\n",
    "\n",
    "**最大似然估计Maximum Likelihood Estimation**\n",
    "\n",
    "通俗理解来说，就是利用已知的样本结果信息，反推最具有可能导致这些样本结果出现的模型参数值。就好比通过一百次摸球实验的结果来推测箱子中不同颜色球的比例。\\\n",
    "对于函数$p(x|\\theta)$：\\\n",
    "如果参数$\\theta$已知，$x$未知，这个函数叫概率函数，它描述对于不同样本点，其出现概率是多少。相反，$\\theta$未知$x$已知就叫似然函数，描述对于不同的模型参数，出现$x$这个样本点的概率是多少。\n",
    "\n",
    "求解最大似然估计的一般过程为：\n",
    "1. 写出似然函数；\n",
    "2. 如果无法直接求导的话，对似然函数取对数；\n",
    "3. 求导数 ；\n",
    "4. 求解模型中参数的最优值。\n",
    "\n",
    "举例：我们现在手里有一枚不知道均不均匀的硬币，设正面概率为$\\theta$，当我们丢了10次硬币，正面出现了7次，出现这样的实验数据的似然函数是多少呢？\n",
    "$$p(x|\\theta)=\\theta^{7}(1-\\theta)^{3}$$\n",
    "最大化似然函数的值，我们得到正面向上概率的值就是0.7。\n",
    "\n",
    "但很多人会说硬币一般都是均匀的啊，是不是有什么别的因素影响了才出这样的结果？那么就要引入最大后验概率了。\n",
    "\n",
    "**最大后验概率Maximum A Posteriori Estimation**\n",
    "\n",
    "最大似然估计是求使似然函数$p(x|\\theta)$最大的参数$\\theta$，而最大后验概率是求使$p(x|\\theta)p(\\theta)$最大的$\\theta$（实际是在最大化后验概率$p(\\theta|x)=p(x|\\theta)p(\\theta)$，$p(x)$是已知的，就把分母去掉了）。也就是说，假如先验分布均匀时，MLE和MAP的估计会得到相同结果。\n",
    "\n",
    "对于硬币的这个例子，我们可以用高斯分布（均值0.5，方差0.1）来描述先验知识，即$p(\\theta)=0.5$的概率更大。那么重新计算得到正面向上概率的值为0.558，并不是实验中的0.7。\n",
    "\n",
    "如何让一个贝叶斯派正视实验结果呢？多实验。当我们实验1000次700次向上时，使用最大后验概率估计得到正面向上概率值为0.696。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "损失函数只考虑了训练集，为了对抗过拟合，增强泛化能力，我们需要向损失函数中加入描述模型复杂程度的正则项$\\Omega(F)$，将经验风险最小化问题转化为结构风险最小化。\n",
    "\n",
    "L1正则化是指权值向量$w$中各个元素的绝对值之和，通常表示为$||w||_1$，产生稀疏模型。\n",
    "$$\\Vert\\vec{x}\\Vert_1=\\sum_{i=1}^N\\vert{x_i}\\vert$$\n",
    "\n",
    "L2正则化是指权值向量$w$中各个元素的平方和然后再求平方根，通常表示为$||w||_2$，防止过拟合。\n",
    "$$\\Vert\\vec{x}\\Vert_2=\\sqrt{\\sum_{i=1}^N{\\vert{x_i}\\vert}^2}$$\n",
    "\n",
    "负无穷范数：向量所有元素中绝对值最小的\n",
    "\n",
    "正无穷番薯：向量所有元素中绝对值最大的\n",
    "\n",
    "p范数：p次方和再开p次方根\n",
    "\n",
    "原因：\n",
    "\n",
    "对于梯度下降法，求解原损失函数$J_0$的过程可以画出等值线，正则化函数$L$也可以在$w^1,w^2$（只考虑二维）中画出来，$J_0$与$L$首次相交的地方就是最优解，L1正则化的函数图像是顶点在坐标轴上的正方形，L2正则化是圆形，通过直观想象我们可以知道L1会有不少权值等于0，而L2发生这种情况的概率会小很多。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
